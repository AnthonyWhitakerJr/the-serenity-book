= Running Serenity tests in parallel

Sometimes projects have a lot of tests, and executing of them takes a lot of time. Testing can be sped up significantly by running different tests in parallel. However, this is often harder to implement than it sounds.

Some build automation tools have builtin parallel test execution, but this not so good for huge amount of tests and heavy tests. For example web tests are as a rule much slower than other types of tests, it make them good candidates for concurrent testing, in theory at least, but the implementation can be tricky. For example, although it is easy enough to configure running tests in parallel, on the other hand running several webdriver instances of Firefox/Chrome in parallel on the same display, tends to become unreliable.

The natural solution in this case is to split the web tests into smaller batches, and to run each batch on a different machine and/or on a different virtual display. When each batch has finished, the results can be retrieved and aggregated into the final test reports.

However splitting tests into manually batches by hand tends to be tedious and unreliable – it is easy to forget to add a new test to a batch, for example, or have unevenly-distributed batches.

Serenity provides mechanisms to automatically split your test suite into slices at runtime, removing the need for any manual structuring of source files or maintenance of scripts. In practice, you decide on the number of batches you require, then run a job associated with each batch number which will deterministically run only tests that belong to that batch. When all jobs have completed running, the test result output is aggregated into the final serenity test report.

== Splitting a Serenity suite into batches

The following parameters affect Serenity's batching behaviour and can be provided at runtime as system parameters or be added to a serenity.properties or serenity.conf file:

*serenity.batch.count*:: Refers to the total number of separately/concurrently running jobs that the whole test suite will be divided up into at runtime. So for instance if you had 8 build slaves available to run your tests, you would set this property to 8. This value would be the same for all jobs running each batch, The property needs to be greater than 1 in order for serenity to trigger its batching behaviour.

*serenity.batch.number*:: This parameter should be different for each batch, and should be a value between 1 and `serenity.batch.count`.

== JUnit-specific configuration

*serenity.batch.strategy*:: Optional parameter for choosing batch weighting strategy (Junit tests only). Possible values are:
 - `DIVIDE_EQUALLY`: Test classes are allocated to batches on a 'round-robin' basis without regard for the number of test methods in each class.
 So for example if `serenity.batch.count` is 3 and we have classes `T1, T2, T3, T4, T5`, the first batch will contain `T1, T4,` the second will contain `T2, T5`, and the third will contain `T3`.
 `DIVIDE_EQUALLY` is the default behaviour when no batch strategy parameter is explicitly set.
 - `DIVIDE_BY_TEST_COUNT`. Classes are allocated to batches based on the number of test methods they contain.
 So for example if `serenity.batch.count` is 2 and class `T1` contains 4 tests, `T2` contains 2 tests, `T3` contains 2 tests, both batches will contain 4 tests each, the first batch containing  `T1`, and the second one containing `T3` and `T2`.
 `DIVIDE_BY_TEST_COUNT` tends to provide more evenly balanced batches than `DIVIDE_EQUALLY`.

== Cucumber-specific configuration

Batching a Cucumber test suite requires a different approach to that used in the JUnit implementation as there is often only a single test runner class used to run the entire suite, so the work cannot be split at the class or test method level.
However, no changes are required to an existing test runner infrastructure, so test runner classes annotated with `@RunWith(CucumberWithSerenity.class)` can still be configured for parallel execution.
Serenity achieves Cucumber test suite batching by analysing the features that are about to be run and splitting the contained scenarios based on their 'weight'.
Serenity calculates scenario 'weight' into `TestStatistics` based on one of the following algorithms:

  - `ScenarioLineCountStatistics`: the number of steps in each scenario (default).
  - `MultiRunTestStatistics`: the historic duration of each scenario when it was run in your CI environment (using one or more Serenity results.csv files).
  This algorithm provides the potential for very accurate slicing, ensuring that all parallel batches have equal run-times and all complete at virtually the same time.

The following parameters can be used to configure Serenity's Cucumber batching behaviour:

*serenity.test.statistics.dir*:: This parameter provides a relative path to the location where one or more serenity results CSV files can be found, taken from recent builds. Any number of results files can be placed in this location and serenity will calculate an averaged scenario weighting.
For instance if you created a directory called `src/test/resources/aggregated` on the file system, the supplied parameter would be `/aggregated`

=== Subdividing a Cucumber batches within a machine

If your build slaves are capable of reliably running tests on more than one browser concurrently, batching can be taken a step further
and work can be divided amongst multiple forked processes. Conceptually, a batch is allocated to machine by means of the `serenity.batch.count` and `serenity.batch.number` parameters described above,
but then the following parameters are used to further subdivide this batch across local forked processes on the build slave:

*serenity.fork.count*:: Refers to the total number of forks that will be created on the build slave. So for instance if you could reliably start 4 browsers on one of your build slaves, you would set this property to 4. This value would be the same for all jobs running each batch, The property needs to be greater than 1 in order for serenity to trigger its batching behaviour.

*serenity.fork.number*:: This parameter will be different for each forked process and will be set automatically by the test runner when it creates the forked processes. It will be a value between 1 and `serenity.batch.count`.

=== Configuring parallel batch execution with Jenkins 1
This approach is easy to set up on Jenkins using a multi-configuration build. In the following screenshot, we are running a multi-configuration build to run web tests across three batches. We use a single user-defined parameter (BATCH_NUMBER) to define the batch being run, passing this parameter into the Maven build job properties we discussed above.

[[fig-multi-configuration-build]]
.Multi-configuration build to run web tests across three batches
image::parallel-webtests-matrix-build.png[]

The most robust way to aggregate the build results from the different batches is to set up a second build job that runs after the test executions, and retrieves the build results from the batch jobs. You can use the Jenkins Copy Artifacts plugin to do this. First, ensure that the multi-configuration build archives the Serenity reports, as shown here:

[[fig-achieving-serenity-reports]]
.Configuration archiving the Serenity reports
image::parallel-webtests-post-build.png[]

This build will then trigger another, freestyle build job. This job needs to copy the Serenity report artifacts from the matrix build jobs into the current workspace, and then run the mvn serenity:aggregate command to generate the Serenity aggregate reports. The matrix build job reports need to be copied one-by-one for each batch, as the current version of the Copy Artifacts plugin does not support copying from multiple projects in the same action.

[[fig-copying-the-serenity-report-artifacts]]
.Configuration copying the Serenity report artifacts and aggregating reports
image::parallel-webtests-aggregate.png[]

Then make sure you publish the generated HTML reports (which will be in the target/site/serenity directory) for easy access to the test results.

This simple example shows a parallel test running 3 batches – this brought the test execution time from 9 minutes to slightly over 1 minute. Results will vary, of course, but a typical real-world set of web tests would have a larger number of batches

=== Configuring parallel batch execution with Jenkins 2 (DSL)
If your CI infrastructure runs on https://jenkins.io/2.0[Jenkins 2] that has the
https://wiki.jenkins-ci.org/display/JENKINS/Pipeline+Plugin[Pipeline] and https://wiki.jenkins-ci.org/display/JENKINS/HTML+Publisher+Plugin[HTML Publisher] plugins installed,
you can quickly define parallel pipelines via a JenkinsFile.

-----
int BATCH_COUNT = 8
int FORK_COUNT = 8
def serenityBatches = [:]

for (int i = 1; i <= BATCH_COUNT; i++) {
    def batchNumber = i
    def batchName = "batch-${batchNumber}"

    serenityBatches[batchName] = {
        node {
            checkout scm
            try {
                mvn "clean"
                sh "rm -rf target/site/serenity"
                mvn "verify -Dit.test=MyTestRunner* -Dserenity.batch.count=${BATCH_COUNT} -Dserenity.batch.count=${FORK_COUNT} -Dserenity.batch.number=${batchNumber} -Dserenity.test.statistics.dir=/statistics -f businessAcceptanceTests/pom.xml"
            } catch (Throwable e) {
                throw e
            } finally {
                stash name: batchName,
                    includes: "target/site/serenity/**/*",
                    allowEmpty: true
            }
        }
    }
}

stage("automated tests") {
    parallel serenityBatches
}

stage("report aggregation") {
    node {
        // unstash each of the batches

        for (batchNumber in BATCH_COUNT) {
            def batchName = "batch-${batchNumber}"
            echo "Unstashing serenity reports for ${batchName}"
            unstash batchName
        }

        // publish the Serenity report

        publishHTML(target: [
                reportName : 'Serenity',
                reportDir:   'target/site/serenity',
                reportFiles: 'index.html',
                keepAll:     true,
                alwaysLinkToLastBuild: true,
                allowMissing: false
        ])
    }
}
-----